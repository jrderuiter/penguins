{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<google.cloud.aiplatform.training_jobs.CustomContainerTrainingJob object at 0x7f1f16720bd0>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "job = aiplatform.CustomContainerTrainingJob(\n",
    "    display_name=\"train-penguin-model\",\n",
    "    command=[\n",
    "        \"penguin-model\", \n",
    "        \"bigquery-public-data.ml_datasets.penguins\"\n",
    "    ],\n",
    "    container_uri=\"europe-west1-docker.pkg.dev/cde-ds-enablement-8k1r/vertex/penguin_model\",\n",
    "    staging_bucket=\"gs://cde-dse-penguin-artifacts\",\n",
    "    location=\"europe-west1\",\n",
    "    model_serving_container_image_uri=\"europe-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.0-24:latest\"\n",
    ")\n",
    "\n",
    "job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.training_jobs:Training Output directory:\n",
      "gs://cde-dse-penguin-artifacts/aiplatform-custom-training-2021-11-08-13:21:02.142 \n",
      "INFO:google.cloud.aiplatform.training_jobs:View Training:\n",
      "https://console.cloud.google.com/ai/platform/locations/europe-west1/training/5078459490743877632?project=409590257761\n",
      "INFO:google.cloud.aiplatform.training_jobs:CustomContainerTrainingJob projects/409590257761/locations/europe-west1/trainingPipelines/5078459490743877632 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:View backing custom job:\n",
      "https://console.cloud.google.com/ai/platform/locations/europe-west1/training/2229932726432038912?project=409590257761\n",
      "INFO:google.cloud.aiplatform.training_jobs:CustomContainerTrainingJob projects/409590257761/locations/europe-west1/trainingPipelines/5078459490743877632 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:CustomContainerTrainingJob projects/409590257761/locations/europe-west1/trainingPipelines/5078459490743877632 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:CustomContainerTrainingJob projects/409590257761/locations/europe-west1/trainingPipelines/5078459490743877632 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_20827/2965956433.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m job.run(\n\u001b[1;32m      2\u001b[0m     \u001b[0mmodel_display_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"penguin-model\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mservice_account\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pipeline-penguin@cde-ds-enablement-8k1r.iam.gserviceaccount.com\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m )\n",
      "\u001b[0;32m~/penguins/.venv/lib/python3.7/site-packages/google/cloud/aiplatform/training_jobs.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, dataset, annotation_schema_uri, model_display_name, model_labels, base_output_dir, service_account, network, bigquery_destination, args, environment_variables, replica_count, machine_type, accelerator_type, accelerator_count, boot_disk_type, boot_disk_size_gb, reduction_server_replica_count, reduction_server_machine_type, reduction_server_container_uri, training_fraction_split, validation_fraction_split, test_fraction_split, training_filter_split, validation_filter_split, test_filter_split, predefined_split_column_name, timestamp_split_column_name, enable_web_access, tensorboard, sync)\u001b[0m\n\u001b[1;32m   2887\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mreduction_server_replica_count\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2888\u001b[0m             \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2889\u001b[0;31m             \u001b[0msync\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msync\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2890\u001b[0m         )\n\u001b[1;32m   2891\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/penguins/.venv/lib/python3.7/site-packages/google/cloud/aiplatform/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    673\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m                     \u001b[0mVertexAiResourceNounWithFutureManager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 675\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    676\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m             \u001b[0;31m# callbacks to call within the Future (in same Thread)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/penguins/.venv/lib/python3.7/site-packages/google/cloud/aiplatform/training_jobs.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, dataset, annotation_schema_uri, worker_pool_specs, managed_model, args, environment_variables, base_output_dir, service_account, network, bigquery_destination, training_fraction_split, validation_fraction_split, test_fraction_split, training_filter_split, validation_filter_split, test_filter_split, predefined_split_column_name, timestamp_split_column_name, enable_web_access, tensorboard, reduction_server_container_uri, sync)\u001b[0m\n\u001b[1;32m   3119\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmanaged_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3120\u001b[0m             \u001b[0mgcs_destination_uri_prefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbase_output_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3121\u001b[0;31m             \u001b[0mbigquery_destination\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbigquery_destination\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3122\u001b[0m         )\n\u001b[1;32m   3123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/penguins/.venv/lib/python3.7/site-packages/google/cloud/aiplatform/training_jobs.py\u001b[0m in \u001b[0;36m_run_job\u001b[0;34m(self, training_task_definition, training_task_inputs, dataset, training_fraction_split, validation_fraction_split, test_fraction_split, training_filter_split, validation_filter_split, test_filter_split, predefined_split_column_name, timestamp_split_column_name, annotation_schema_uri, model, gcs_destination_uri_prefix, bigquery_destination)\u001b[0m\n\u001b[1;32m    749\u001b[0m         \u001b[0m_LOGGER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"View Training:\\n%s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dashboard_uri\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/penguins/.venv/lib/python3.7/site-packages/google/cloud/aiplatform/training_jobs.py\u001b[0m in \u001b[0;36m_get_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    836\u001b[0m             \u001b[0mRuntimeError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIf\u001b[0m \u001b[0mTraining\u001b[0m \u001b[0mfailed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    837\u001b[0m         \"\"\"\n\u001b[0;32m--> 838\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_block_until_complete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    839\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_failed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/penguins/.venv/lib/python3.7/site-packages/google/cloud/aiplatform/training_jobs.py\u001b[0m in \u001b[0;36m_block_until_complete\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    884\u001b[0m                 \u001b[0mprevious_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurrent_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    885\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_callback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 886\u001b[0;31m             \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_failure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "job.run(\n",
    "    model_display_name=\"penguin-model\",\n",
    "    service_account=\"pipeline-penguin@cde-ds-enablement-8k1r.iam.gserviceaccount.com\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m\n",
      "\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcloud\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maiplatform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcloud\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maiplatform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtabular_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTabularDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcloud\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maiplatform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcloud\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maiplatform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvideo_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVideoDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mannotation_schema_uri\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmodel_display_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmodel_labels\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mbase_output_dir\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mservice_account\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mnetwork\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mbigquery_destination\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0menvironment_variables\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mreplica_count\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmachine_type\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'n1-standard-4'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0maccelerator_type\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'ACCELERATOR_TYPE_UNSPECIFIED'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0maccelerator_count\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mboot_disk_type\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'pd-ssd'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mboot_disk_size_gb\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mreduction_server_replica_count\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mreduction_server_machine_type\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mreduction_server_container_uri\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mtraining_fraction_split\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mvalidation_fraction_split\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mtest_fraction_split\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mtraining_filter_split\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mvalidation_filter_split\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mtest_filter_split\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mpredefined_split_column_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mtimestamp_split_column_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0menable_web_access\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mtensorboard\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0msync\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcloud\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maiplatform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "Runs the custom training job.\n",
      "\n",
      "Distributed Training Support:\n",
      "If replica count = 1 then one chief replica will be provisioned. If\n",
      "replica_count > 1 the remainder will be provisioned as a worker replica pool.\n",
      "ie: replica_count = 10 will result in 1 chief and 9 workers\n",
      "All replicas have same machine_type, accelerator_type, and accelerator_count\n",
      "\n",
      "If training on a Vertex AI dataset, you can use one of the following split configurations:\n",
      "    Data fraction splits:\n",
      "    Any of ``training_fraction_split``, ``validation_fraction_split`` and\n",
      "    ``test_fraction_split`` may optionally be provided, they must sum to up to 1. If\n",
      "    the provided ones sum to less than 1, the remainder is assigned to sets as\n",
      "    decided by Vertex AI. If none of the fractions are set, by default roughly 80%\n",
      "    of data will be used for training, 10% for validation, and 10% for test.\n",
      "\n",
      "    Data filter splits:\n",
      "    Assigns input data to training, validation, and test sets\n",
      "    based on the given filters, data pieces not matched by any\n",
      "    filter are ignored. Currently only supported for Datasets\n",
      "    containing DataItems.\n",
      "    If any of the filters in this message are to match nothing, then\n",
      "    they can be set as '-' (the minus sign).\n",
      "    If using filter splits, all of ``training_filter_split``, ``validation_filter_split`` and\n",
      "    ``test_filter_split`` must be provided.\n",
      "    Supported only for unstructured Datasets.\n",
      "\n",
      "    Predefined splits:\n",
      "    Assigns input data to training, validation, and test sets based on the value of a provided key.\n",
      "    If using predefined splits, ``predefined_split_column_name`` must be provided.\n",
      "    Supported only for tabular Datasets.\n",
      "\n",
      "    Timestamp splits:\n",
      "    Assigns input data to training, validation, and test sets\n",
      "    based on a provided timestamps. The youngest data pieces are\n",
      "    assigned to training set, next to validation set, and the oldest\n",
      "    to the test set.\n",
      "    Supported only for tabular Datasets.\n",
      "\n",
      "Args:\n",
      "    dataset (Union[datasets.ImageDataset,datasets.TabularDataset,datasets.TextDataset,datasets.VideoDataset]):\n",
      "        Vertex AI to fit this training against. Custom training script should\n",
      "        retrieve datasets through passed in environment variables uris:\n",
      "\n",
      "        os.environ[\"AIP_TRAINING_DATA_URI\"]\n",
      "        os.environ[\"AIP_VALIDATION_DATA_URI\"]\n",
      "        os.environ[\"AIP_TEST_DATA_URI\"]\n",
      "\n",
      "        Additionally the dataset format is passed in as:\n",
      "\n",
      "        os.environ[\"AIP_DATA_FORMAT\"]\n",
      "    annotation_schema_uri (str):\n",
      "        Google Cloud Storage URI points to a YAML file describing\n",
      "        annotation schema. The schema is defined as an OpenAPI 3.0.2\n",
      "        [Schema Object](https://github.com/OAI/OpenAPI-Specification/blob/main/versions/3.0.2.md#schema-object) The schema files\n",
      "        that can be used here are found in\n",
      "        gs://google-cloud-aiplatform/schema/dataset/annotation/,\n",
      "        note that the chosen schema must be consistent with\n",
      "        ``metadata``\n",
      "        of the Dataset specified by\n",
      "        ``dataset_id``.\n",
      "\n",
      "        Only Annotations that both match this schema and belong to\n",
      "        DataItems not ignored by the split method are used in\n",
      "        respectively training, validation or test role, depending on\n",
      "        the role of the DataItem they are on.\n",
      "\n",
      "        When used in conjunction with\n",
      "        ``annotations_filter``,\n",
      "        the Annotations used for training are filtered by both\n",
      "        ``annotations_filter``\n",
      "        and\n",
      "        ``annotation_schema_uri``.\n",
      "    model_display_name (str):\n",
      "        If the script produces a managed Vertex AI Model. The display name of\n",
      "        the Model. The name can be up to 128 characters long and can be consist\n",
      "        of any UTF-8 characters.\n",
      "\n",
      "        If not provided upon creation, the job's display_name is used.\n",
      "    model_labels (Dict[str, str]):\n",
      "        Optional. The labels with user-defined metadata to\n",
      "        organize your Models.\n",
      "        Label keys and values can be no longer than 64\n",
      "        characters (Unicode codepoints), can only\n",
      "        contain lowercase letters, numeric characters,\n",
      "        underscores and dashes. International characters\n",
      "        are allowed.\n",
      "        See https://goo.gl/xmQnxf for more information\n",
      "        and examples of labels.\n",
      "    base_output_dir (str):\n",
      "        GCS output directory of job. If not provided a\n",
      "        timestamped directory in the staging directory will be used.\n",
      "\n",
      "        Vertex AI sets the following environment variables when it runs your training code:\n",
      "\n",
      "        -  AIP_MODEL_DIR: a Cloud Storage URI of a directory intended for saving model artifacts, i.e. <base_output_dir>/model/\n",
      "        -  AIP_CHECKPOINT_DIR: a Cloud Storage URI of a directory intended for saving checkpoints, i.e. <base_output_dir>/checkpoints/\n",
      "        -  AIP_TENSORBOARD_LOG_DIR: a Cloud Storage URI of a directory intended for saving TensorBoard logs, i.e. <base_output_dir>/logs/\n",
      "\n",
      "    service_account (str):\n",
      "        Specifies the service account for workload run-as account.\n",
      "        Users submitting jobs must have act-as permission on this run-as account.\n",
      "    network (str):\n",
      "        The full name of the Compute Engine network to which the job\n",
      "        should be peered. For example, projects/12345/global/networks/myVPC.\n",
      "        Private services access must already be configured for the network.\n",
      "        If left unspecified, the job is not peered with any network.\n",
      "    bigquery_destination (str):\n",
      "        Provide this field if `dataset` is a BiqQuery dataset.\n",
      "        The BigQuery project location where the training data is to\n",
      "        be written to. In the given project a new dataset is created\n",
      "        with name\n",
      "        ``dataset_<dataset-id>_<annotation-type>_<timestamp-of-training-call>``\n",
      "        where timestamp is in YYYY_MM_DDThh_mm_ss_sssZ format. All\n",
      "        training input data will be written into that dataset. In\n",
      "        the dataset three tables will be created, ``training``,\n",
      "        ``validation`` and ``test``.\n",
      "\n",
      "        -  AIP_DATA_FORMAT = \"bigquery\".\n",
      "        -  AIP_TRAINING_DATA_URI =\"bigquery_destination.dataset_*.training\"\n",
      "        -  AIP_VALIDATION_DATA_URI = \"bigquery_destination.dataset_*.validation\"\n",
      "        -  AIP_TEST_DATA_URI = \"bigquery_destination.dataset_*.test\"\n",
      "    args (List[Unions[str, int, float]]):\n",
      "        Command line arguments to be passed to the Python script.\n",
      "    environment_variables (Dict[str, str]):\n",
      "        Environment variables to be passed to the container.\n",
      "        Should be a dictionary where keys are environment variable names\n",
      "        and values are environment variable values for those names.\n",
      "        At most 10 environment variables can be specified.\n",
      "        The Name of the environment variable must be unique.\n",
      "\n",
      "        environment_variables = {\n",
      "            'MY_KEY': 'MY_VALUE'\n",
      "        }\n",
      "    replica_count (int):\n",
      "        The number of worker replicas. If replica count = 1 then one chief\n",
      "        replica will be provisioned. If replica_count > 1 the remainder will be\n",
      "        provisioned as a worker replica pool.\n",
      "    machine_type (str):\n",
      "        The type of machine to use for training.\n",
      "    accelerator_type (str):\n",
      "        Hardware accelerator type. One of ACCELERATOR_TYPE_UNSPECIFIED,\n",
      "        NVIDIA_TESLA_K80, NVIDIA_TESLA_P100, NVIDIA_TESLA_V100, NVIDIA_TESLA_P4,\n",
      "        NVIDIA_TESLA_T4\n",
      "    accelerator_count (int):\n",
      "        The number of accelerators to attach to a worker replica.\n",
      "    boot_disk_type (str):\n",
      "        Type of the boot disk, default is `pd-ssd`.\n",
      "        Valid values: `pd-ssd` (Persistent Disk Solid State Drive) or\n",
      "        `pd-standard` (Persistent Disk Hard Disk Drive).\n",
      "    boot_disk_size_gb (int):\n",
      "        Size in GB of the boot disk, default is 100GB.\n",
      "        boot disk size must be within the range of [100, 64000].\n",
      "    reduction_server_replica_count (int):\n",
      "        The number of reduction server replicas, default is 0.\n",
      "    reduction_server_machine_type (str):\n",
      "        Optional. The type of machine to use for reduction server.\n",
      "    reduction_server_container_uri (str):\n",
      "        Optional. The Uri of the reduction server container image.\n",
      "        See details: https://cloud.google.com/vertex-ai/docs/training/distributed-training#reduce_training_time_with_reduction_server\n",
      "    training_fraction_split (float):\n",
      "        Optional. The fraction of the input data that is to be used to train\n",
      "        the Model. This is ignored if Dataset is not provided.\n",
      "    validation_fraction_split (float):\n",
      "        Optional. The fraction of the input data that is to be used to validate\n",
      "        the Model. This is ignored if Dataset is not provided.\n",
      "    test_fraction_split (float):\n",
      "        Optional. The fraction of the input data that is to be used to evaluate\n",
      "        the Model. This is ignored if Dataset is not provided.\n",
      "    training_filter_split (str):\n",
      "        Optional. A filter on DataItems of the Dataset. DataItems that match\n",
      "        this filter are used to train the Model. A filter with same syntax\n",
      "        as the one used in DatasetService.ListDataItems may be used. If a\n",
      "        single DataItem is matched by more than one of the FilterSplit filters,\n",
      "        then it is assigned to the first set that applies to it in the training,\n",
      "        validation, test order. This is ignored if Dataset is not provided.\n",
      "    validation_filter_split (str):\n",
      "        Optional. A filter on DataItems of the Dataset. DataItems that match\n",
      "        this filter are used to validate the Model. A filter with same syntax\n",
      "        as the one used in DatasetService.ListDataItems may be used. If a\n",
      "        single DataItem is matched by more than one of the FilterSplit filters,\n",
      "        then it is assigned to the first set that applies to it in the training,\n",
      "        validation, test order. This is ignored if Dataset is not provided.\n",
      "    test_filter_split (str):\n",
      "        Optional. A filter on DataItems of the Dataset. DataItems that match\n",
      "        this filter are used to test the Model. A filter with same syntax\n",
      "        as the one used in DatasetService.ListDataItems may be used. If a\n",
      "        single DataItem is matched by more than one of the FilterSplit filters,\n",
      "        then it is assigned to the first set that applies to it in the training,\n",
      "        validation, test order. This is ignored if Dataset is not provided.\n",
      "    predefined_split_column_name (str):\n",
      "        Optional. The key is a name of one of the Dataset's data\n",
      "        columns. The value of the key (either the label's value or\n",
      "        value in the column) must be one of {``training``,\n",
      "        ``validation``, ``test``}, and it defines to which set the\n",
      "        given piece of data is assigned. If for a piece of data the\n",
      "        key is not present or has an invalid value, that piece is\n",
      "        ignored by the pipeline.\n",
      "\n",
      "        Supported only for tabular and time series Datasets.\n",
      "    timestamp_split_column_name (str):\n",
      "        Optional. The key is a name of one of the Dataset's data\n",
      "        columns. The value of the key values of the key (the values in\n",
      "        the column) must be in RFC 3339 `date-time` format, where\n",
      "        `time-offset` = `\"Z\"` (e.g. 1985-04-12T23:20:50.52Z). If for a\n",
      "        piece of data the key is not present or has an invalid value,\n",
      "        that piece is ignored by the pipeline.\n",
      "\n",
      "        Supported only for tabular and time series Datasets.\n",
      "    enable_web_access (bool):\n",
      "        Whether you want Vertex AI to enable interactive shell access\n",
      "        to training containers.\n",
      "        https://cloud.google.com/vertex-ai/docs/training/monitor-debug-interactive-shell\n",
      "    tensorboard (str):\n",
      "        Optional. The name of a Vertex AI\n",
      "        [Tensorboard][google.cloud.aiplatform.v1beta1.Tensorboard]\n",
      "        resource to which this CustomJob will upload Tensorboard\n",
      "        logs. Format:\n",
      "        ``projects/{project}/locations/{location}/tensorboards/{tensorboard}``\n",
      "\n",
      "        The training script should write Tensorboard to following Vertex AI environment\n",
      "        variable:\n",
      "\n",
      "        AIP_TENSORBOARD_LOG_DIR\n",
      "\n",
      "        `service_account` is required with provided `tensorboard`.\n",
      "        For more information on configuring your service account please visit:\n",
      "        https://cloud.google.com/vertex-ai/docs/experiments/tensorboard-training\n",
      "    sync (bool):\n",
      "        Whether to execute this method synchronously. If False, this method\n",
      "        will be executed in concurrent Future and any downstream object will\n",
      "        be immediately returned and synced when the Future has completed.\n",
      "\n",
      "Returns:\n",
      "    model: The trained Vertex AI Model resource or None if training did not\n",
      "        produce a Vertex AI Model.\n",
      "\n",
      "Raises:\n",
      "    RuntimeError: If Training job has already been run, staging_bucket has not\n",
      "        been set, or model_display_name was provided but required arguments\n",
      "        were not provided in constructor.\n",
      "\u001b[0;31mFile:\u001b[0m      ~/penguins/.venv/lib/python3.7/site-packages/google/cloud/aiplatform/training_jobs.py\n",
      "\u001b[0;31mType:\u001b[0m      method\n"
     ]
    }
   ],
   "source": [
    "job.run?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'type' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_20827/3139812547.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m )\n\u001b[1;32m     23\u001b[0m def query(\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mquery\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_path\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Parquet\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproject_id\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m ) -> None:\n\u001b[1;32m     26\u001b[0m     \u001b[0;34m\"\"\"Calculates sum of two arguments\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'type' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "from typing import Optional, NamedTuple\n",
    "\n",
    "import kfp\n",
    "from kfp.v2.google import experimental\n",
    "import kfp.components as comp\n",
    "from kfp.v2 import compiler  # noqa: F811\n",
    "from kfp.v2.dsl import (\n",
    "    component,\n",
    "    Input,\n",
    "    InputPath,\n",
    "    OutputPath,\n",
    "    Output,\n",
    "    Dataset,\n",
    "    Metrics,\n",
    ")\n",
    "\n",
    "\n",
    "@component(\n",
    "    base_image=\"python:3.9-slim\",\n",
    "    packages_to_install=[\"google-cloud-bigquery\", \"pandas\", \"pyarrow\"],\n",
    "    output_component_file=\"query.yaml\"\n",
    ")\n",
    "def query(\n",
    "    query: str, output_path: Dataset[\"Parquet\"], project_id: Optional[str] = None\n",
    ") -> None:\n",
    "    \"\"\"Calculates sum of two arguments\"\"\"\n",
    "\n",
    "    from google.cloud import bigquery\n",
    "\n",
    "    client = bigquery.Client(project=project_id)\n",
    "    job = client.query(query)\n",
    "\n",
    "    df = job.to_dataframe()\n",
    "    df.to_parquet(output_path)\n",
    "\n",
    "\n",
    "# query = comp.create_component_from_func(\n",
    "#     _query,\n",
    "#     base_image=\"python:3.9-slim\",\n",
    "#     packages_to_install=[\"google-cloud-bigquery\", \"pandas\", \"pyarrow\"],\n",
    "#     output_component_file=\"query.yaml\",\n",
    "# )\n",
    "\n",
    "\n",
    "train_model = comp.load_component_from_text(\n",
    "    \"\"\"\n",
    "name: Train model\n",
    "description: Trains our model\n",
    "\n",
    "inputs:\n",
    "- {name: train_dataset, type: Parquet, description: 'Train dataset'}\n",
    "\n",
    "outputs:\n",
    "- {name: model, type: Model, description: 'Output model'}\n",
    "\n",
    "implementation:\n",
    "  container:\n",
    "    image: europe-west1-docker.pkg.dev/cde-ds-enablement-8k1r/vertex/penguin_model\n",
    "    command: [\n",
    "      penguin-model, \n",
    "      {inputPath: train_dataset},\n",
    "      {outputPath: model}\n",
    "    ]\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "@component(\n",
    "    base_image=\"python:3.9-slim\",\n",
    "    output_component_file=\"eval_model.yaml\"\n",
    ")\n",
    "def eval_model(model_path: InputPath(\"Model\"), metrics: Output[Metrics]) -> NamedTuple(\n",
    "    'EvalModelOutput',\n",
    "    [\n",
    "      ('roc', float)\n",
    "    ]):\n",
    "  print(model_path)\n",
    "\n",
    "  metrics.log_metric(\"roc\", 0.9)\n",
    "  \n",
    "\n",
    "@kfp.dsl.pipeline(name=\"penguin\")\n",
    "def pipeline():\n",
    "\n",
    "    query_task = query(\n",
    "        \"SELECT * FROM bigquery-public-data.ml_datasets.penguins\",\n",
    "        project_id=\"cde-ds-enablement-8k1r\",\n",
    "    )\n",
    "\n",
    "    train_task = (\n",
    "        train_model(query_task.outputs[\"output_path\"])\n",
    "        # TODO: Figure out how to set constaints in the v2 API.\n",
    "        # Docs: https://www.kubeflow.org/docs/distributions/gke/pipelines/enable-gpu-and-tpu/\n",
    "        # .set_gpu_limit(1).add_node_selector_constraint(\n",
    "        #     \"cloud.google.com/gke-accelerator\", \"nvidia-tesla-k80\"\n",
    "        # )\n",
    "    )\n",
    "\n",
    "    eval_task = eval_model(train_task.outputs[\"model\"])\n",
    "\n",
    "    # model_upload_op = gcc_aip.ModelUploadOp(\n",
    "    #     project=project,\n",
    "    #     display_name=model_display_name,\n",
    "    #     artifact_uri=WORKING_DIR,\n",
    "    #     serving_container_image_uri=serving_container_image_uri,\n",
    "    #     # serving_container_environment_variables={\"NOT_USED\": \"NO_VALUE\"},\n",
    "    # )\n",
    "    # model_upload_op.after(train_task)\n",
    "\n",
    "\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=pipeline,\n",
    "    package_path=\"pipeline.json\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:Creating PipelineJob\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob created. Resource name: projects/409590257761/locations/europe-west1/pipelineJobs/penguin-20211108161424\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:To use this PipelineJob in another session:\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:pipeline_job = aiplatform.PipelineJob.get('projects/409590257761/locations/europe-west1/pipelineJobs/penguin-20211108161424')\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/europe-west1/pipelines/runs/penguin-20211108161424?project=409590257761\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/409590257761/locations/europe-west1/pipelineJobs/penguin-20211108161424 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/409590257761/locations/europe-west1/pipelineJobs/penguin-20211108161424 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/409590257761/locations/europe-west1/pipelineJobs/penguin-20211108161424 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/409590257761/locations/europe-west1/pipelineJobs/penguin-20211108161424 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_20827/1444885540.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m )\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mservice_account\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pipeline-penguin@cde-ds-enablement-8k1r.iam.gserviceaccount.com\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/penguins/.venv/lib/python3.7/site-packages/google/cloud/aiplatform/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    673\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m                     \u001b[0mVertexAiResourceNounWithFutureManager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 675\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    676\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m             \u001b[0;31m# callbacks to call within the Future (in same Thread)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/penguins/.venv/lib/python3.7/site-packages/google/cloud/aiplatform/pipeline_jobs.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, service_account, network, sync)\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubmit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mservice_account\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mservice_account\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_block_until_complete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m     def submit(\n",
      "\u001b[0;32m~/penguins/.venv/lib/python3.7/site-packages/google/cloud/aiplatform/pipeline_jobs.py\u001b[0m in \u001b[0;36m_block_until_complete\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    344\u001b[0m                 \u001b[0mlog_wait\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_wait\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmultiplier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_wait\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m                 \u001b[0mprevious_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurrent_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m             \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m         \u001b[0;31m# Error is only populated when the job state is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from google.cloud.aiplatform.pipeline_jobs import PipelineJob\n",
    "\n",
    "job = PipelineJob(\n",
    "    display_name=\"penguins\",\n",
    "    enable_caching=False,\n",
    "    template_path=\"pipeline.json\",\n",
    "    parameter_values={},\n",
    "    pipeline_root=\"gs://cde-dse-penguin-artifacts/pipelines\",\n",
    "    location=\"europe-west1\",\n",
    ")\n",
    "\n",
    "job.run(service_account=\"pipeline-penguin@cde-ds-enablement-8k1r.iam.gserviceaccount.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.training_jobs:Training Output directory:\n",
      "gs://cde-dse-penguin-artifacts/aiplatform-custom-training-2021-11-08-13:21:02.142 \n",
      "INFO:google.cloud.aiplatform.training_jobs:View Training:\n",
      "https://console.cloud.google.com/ai/platform/locations/europe-west1/training/5078459490743877632?project=409590257761\n",
      "INFO:google.cloud.aiplatform.training_jobs:CustomContainerTrainingJob projects/409590257761/locations/europe-west1/trainingPipelines/5078459490743877632 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:View backing custom job:\n",
      "https://console.cloud.google.com/ai/platform/locations/europe-west1/training/2229932726432038912?project=409590257761\n",
      "INFO:google.cloud.aiplatform.training_jobs:CustomContainerTrainingJob projects/409590257761/locations/europe-west1/trainingPipelines/5078459490743877632 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:CustomContainerTrainingJob projects/409590257761/locations/europe-west1/trainingPipelines/5078459490743877632 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:CustomContainerTrainingJob projects/409590257761/locations/europe-west1/trainingPipelines/5078459490743877632 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)\n",
      "\u001b[0;32m/tmp/ipykernel_20827/2965956433.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[1;32m      1\u001b[0m job.run(\n",
      "\u001b[1;32m      2\u001b[0m     \u001b[0mmodel_display_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"penguin-model\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mservice_account\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pipeline-penguin@cde-ds-enablement-8k1r.iam.gserviceaccount.com\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m      4\u001b[0m )\n",
      "\n",
      "\u001b[0;32m~/penguins/.venv/lib/python3.7/site-packages/google/cloud/aiplatform/training_jobs.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, dataset, annotation_schema_uri, model_display_name, model_labels, base_output_dir, service_account, network, bigquery_destination, args, environment_variables, replica_count, machine_type, accelerator_type, accelerator_count, boot_disk_type, boot_disk_size_gb, reduction_server_replica_count, reduction_server_machine_type, reduction_server_container_uri, training_fraction_split, validation_fraction_split, test_fraction_split, training_filter_split, validation_filter_split, test_filter_split, predefined_split_column_name, timestamp_split_column_name, enable_web_access, tensorboard, sync)\u001b[0m\n",
      "\u001b[1;32m   2887\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mreduction_server_replica_count\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m   2888\u001b[0m             \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m-> 2889\u001b[0;31m             \u001b[0msync\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msync\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m   2890\u001b[0m         )\n",
      "\u001b[1;32m   2891\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\n",
      "\u001b[0;32m~/penguins/.venv/lib/python3.7/site-packages/google/cloud/aiplatform/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m    673\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    674\u001b[0m                     \u001b[0mVertexAiResourceNounWithFutureManager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m--> 675\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m    676\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    677\u001b[0m             \u001b[0;31m# callbacks to call within the Future (in same Thread)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\n",
      "\u001b[0;32m~/penguins/.venv/lib/python3.7/site-packages/google/cloud/aiplatform/training_jobs.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, dataset, annotation_schema_uri, worker_pool_specs, managed_model, args, environment_variables, base_output_dir, service_account, network, bigquery_destination, training_fraction_split, validation_fraction_split, test_fraction_split, training_filter_split, validation_filter_split, test_filter_split, predefined_split_column_name, timestamp_split_column_name, enable_web_access, tensorboard, reduction_server_container_uri, sync)\u001b[0m\n",
      "\u001b[1;32m   3119\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmanaged_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m   3120\u001b[0m             \u001b[0mgcs_destination_uri_prefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbase_output_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m-> 3121\u001b[0;31m             \u001b[0mbigquery_destination\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbigquery_destination\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m   3122\u001b[0m         )\n",
      "\u001b[1;32m   3123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\n",
      "\u001b[0;32m~/penguins/.venv/lib/python3.7/site-packages/google/cloud/aiplatform/training_jobs.py\u001b[0m in \u001b[0;36m_run_job\u001b[0;34m(self, training_task_definition, training_task_inputs, dataset, training_fraction_split, validation_fraction_split, test_fraction_split, training_filter_split, validation_filter_split, test_filter_split, predefined_split_column_name, timestamp_split_column_name, annotation_schema_uri, model, gcs_destination_uri_prefix, bigquery_destination)\u001b[0m\n",
      "\u001b[1;32m    749\u001b[0m         \u001b[0m_LOGGER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"View Training:\\n%s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dashboard_uri\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    750\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m--> 751\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m    752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    753\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\n",
      "\u001b[0;32m~/penguins/.venv/lib/python3.7/site-packages/google/cloud/aiplatform/training_jobs.py\u001b[0m in \u001b[0;36m_get_model\u001b[0;34m(self)\u001b[0m\n",
      "\u001b[1;32m    836\u001b[0m             \u001b[0mRuntimeError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIf\u001b[0m \u001b[0mTraining\u001b[0m \u001b[0mfailed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    837\u001b[0m         \"\"\"\n",
      "\u001b[0;32m--> 838\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_block_until_complete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m    839\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    840\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_failed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\n",
      "\u001b[0;32m~/penguins/.venv/lib/python3.7/site-packages/google/cloud/aiplatform/training_jobs.py\u001b[0m in \u001b[0;36m_block_until_complete\u001b[0;34m(self)\u001b[0m\n",
      "\u001b[1;32m    884\u001b[0m                 \u001b[0mprevious_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurrent_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    885\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_callback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m--> 886\u001b[0;31m             \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    888\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_failure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "job.run(\n",
    "    model_display_name=\"penguin-model\",\n",
    "    service_account=\"pipeline-penguin@cde-ds-enablement-8k1r.iam.gserviceaccount.com\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mInit signature:\u001b[0m\n",
      "\u001b[0mPipelineJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdisplay_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mtemplate_path\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mjob_id\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mpipeline_root\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mparameter_values\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0menable_caching\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mencryption_spec_key_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mlabels\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mcredentials\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcredentials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCredentials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mproject\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mlocation\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m     \n",
      "Allows optional asynchronous calls to this Vertex AI Resource\n",
      "Nouns.\n",
      "\u001b[0;31mInit docstring:\u001b[0m\n",
      "Retrieves a PipelineJob resource and instantiates its\n",
      "representation.\n",
      "\n",
      "Args:\n",
      "    display_name (str):\n",
      "        Required. The user-defined name of this Pipeline.\n",
      "    template_path (str):\n",
      "        Required. The path of PipelineJob or PipelineSpec JSON file. It\n",
      "        can be a local path or a Google Cloud Storage URI.\n",
      "        Example: \"gs://project.name\"\n",
      "    job_id (str):\n",
      "        Optional. The unique ID of the job run.\n",
      "        If not specified, pipeline name + timestamp will be used.\n",
      "    pipeline_root (str):\n",
      "        Optional. The root of the pipeline outputs. Default to be staging bucket.\n",
      "    parameter_values (Dict[str, Any]):\n",
      "        Optional. The mapping from runtime parameter names to its values that\n",
      "        control the pipeline run.\n",
      "    enable_caching (bool):\n",
      "        Optional. Whether to turn on caching for the run.\n",
      "\n",
      "        If this is not set, defaults to the compile time settings, which\n",
      "        are True for all tasks by default, while users may specify\n",
      "        different caching options for individual tasks.\n",
      "\n",
      "        If this is set, the setting applies to all tasks in the pipeline.\n",
      "\n",
      "        Overrides the compile time settings.\n",
      "    encryption_spec_key_name (str):\n",
      "        Optional. The Cloud KMS resource identifier of the customer\n",
      "        managed encryption key used to protect the job. Has the\n",
      "        form:\n",
      "        ``projects/my-project/locations/my-region/keyRings/my-kr/cryptoKeys/my-key``.\n",
      "        The key needs to be in the same region as where the compute\n",
      "        resource is created.\n",
      "\n",
      "        If this is set, then all\n",
      "        resources created by the PipelineJob will\n",
      "        be encrypted with the provided encryption key.\n",
      "\n",
      "        Overrides encryption_spec_key_name set in aiplatform.init.\n",
      "    labels (Dict[str,str]):\n",
      "        Optional. The user defined metadata to organize PipelineJob.\n",
      "    credentials (auth_credentials.Credentials):\n",
      "        Optional. Custom credentials to use to create this batch prediction\n",
      "        job. Overrides credentials set in aiplatform.init.\n",
      "    project (str),\n",
      "        Optional. Project to retrieve PipelineJob from. If not set,\n",
      "        project set in aiplatform.init will be used.\n",
      "    location (str),\n",
      "        Optional. Location to create PipelineJob. If not set,\n",
      "        location set in aiplatform.init will be used.\n",
      "\n",
      "Raises:\n",
      "    ValueError: If job_id or labels have incorrect format.\n",
      "\u001b[0;31mFile:\u001b[0m           ~/penguins/.venv/lib/python3.7/site-packages/google/cloud/aiplatform/pipeline_jobs.py\n",
      "\u001b[0;31mType:\u001b[0m           ABCMeta\n",
      "\u001b[0;31mSubclasses:\u001b[0m     \n"
     ]
    }
   ],
   "source": [
    "PipelineJob?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m\n",
      "\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_as_aiplatform_custom_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mop\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mkfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdsl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_container_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mContainerOp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdisplay_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mreplica_count\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmachine_type\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0maccelerator_type\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0maccelerator_count\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mboot_disk_type\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mboot_disk_size_gb\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mrestart_job_on_worker_restart\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mservice_account\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mnetwork\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0moutput_uri_prefix\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mworker_pool_specs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mMapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "Run a pipeline task using AI Platform (Unified) custom training job.\n",
      "\n",
      "For detailed doc of the service, please refer to\n",
      "https://cloud.google.com/ai-platform-unified/docs/training/create-custom-job\n",
      "\n",
      "Args:\n",
      "  op: The task (ContainerOp) object to run as aiplatform custom job.\n",
      "  display_name: Optional. The name of the custom job.\n",
      "  replica_count: Optional. The number of replicas to be split between master\n",
      "    workerPoolSpec and worker workerPoolSpec. (master always has 1 replica).\n",
      "  machine_type: Optional. The type of the machine to run the custom job. The\n",
      "    default value is \"n1-standard-4\".\n",
      "  accelerator_type: Optional. The type of accelerator(s) that may be attached\n",
      "    to the machine as per acceleratorCount. Optional.\n",
      "  accelerator_count: Optional. The number of accelerators to attach to the\n",
      "    machine.\n",
      "  boot_disk_type: Optional. Type of the boot disk (default is \"pd-ssd\"). Valid\n",
      "    values: \"pd-ssd\" (Persistent Disk Solid State Drive) or \"pd-standard\"\n",
      "      (Persistent Disk Hard Disk Drive).\n",
      "  boot_disk_size_gb: Optional. Size in GB of the boot disk (default is 100GB).\n",
      "  timeout: Optional. The maximum job running time. The default is 7 days. A\n",
      "    duration in seconds with up to nine fractional digits, terminated by 's'.\n",
      "    Example: \"3.5s\"\n",
      "  restart_job_on_worker_restart: Optional. Restarts the entire CustomJob if a\n",
      "    worker gets restarted. This feature can be used by distributed training\n",
      "    jobs that are not resilient to workers leaving and joining a job.\n",
      "  service_account: Optional. Specifies the service account for workload run-as\n",
      "    account.\n",
      "  network: Optional. The full name of the Compute Engine network to which the\n",
      "    job should be peered. For example, projects/12345/global/networks/myVPC.\n",
      "  output_uri_prefix: Optional. Google Cloud Storage URI to output directory.\n",
      "  additional_worker_pool_specs: Optional. Additional workerPoolSpecs for\n",
      "    distributed training. For details, please see:\n",
      "    https://cloud.google.com/ai-platform-unified/docs/training/distributed-training\n",
      "\u001b[0;31mFile:\u001b[0m      ~/penguins/.venv/lib/python3.7/site-packages/kfp/v2/google/experimental/custom_job.py\n",
      "\u001b[0;31mType:\u001b[0m      function\n"
     ]
    }
   ],
   "source": [
    "experimental.run_as_aiplatform_custom_job?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

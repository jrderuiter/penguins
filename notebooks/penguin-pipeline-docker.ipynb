{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "job = aiplatform.CustomContainerTrainingJob(\n",
    "    display_name=\"train-penguin-model\",\n",
    "    command=[\n",
    "        \"penguin-model\", \n",
    "        \"bigquery-public-data.ml_datasets.penguins\"\n",
    "    ],\n",
    "    container_uri=\"europe-west1-docker.pkg.dev/cde-ds-enablement-8k1r/vertex/penguin_model\",\n",
    "    staging_bucket=\"gs://cde-dse-penguin-artifacts\",\n",
    "    location=\"europe-west1\",\n",
    "    model_serving_container_image_uri=\"europe-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.0-24:latest\"\n",
    ")\n",
    "\n",
    "job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job.run(\n",
    "    model_display_name=\"penguin-model\",\n",
    "    service_account=\"pipeline-penguin@cde-ds-enablement-8k1r.iam.gserviceaccount.com\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job.run?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, NamedTuple\n",
    "\n",
    "import kfp\n",
    "from kfp.v2.google import experimental\n",
    "import kfp.components as comp\n",
    "from kfp.v2 import compiler  # noqa: F811\n",
    "from kfp.v2.dsl import (\n",
    "    component,\n",
    "    Input,\n",
    "    InputPath,\n",
    "    OutputPath,\n",
    "    Output,\n",
    "    Dataset,\n",
    "    Metrics,\n",
    ")\n",
    "\n",
    "\n",
    "@component(\n",
    "    base_image=\"python:3.9-slim\",\n",
    "    packages_to_install=[\"google-cloud-bigquery\", \"pandas\", \"pyarrow\"],\n",
    "    output_component_file=\"query.yaml\"\n",
    ")\n",
    "def query(\n",
    "    query: str, output_path: Dataset[\"Parquet\"], project_id: Optional[str] = None\n",
    ") -> None:\n",
    "    \"\"\"Calculates sum of two arguments\"\"\"\n",
    "\n",
    "    from google.cloud import bigquery\n",
    "\n",
    "    client = bigquery.Client(project=project_id)\n",
    "    job = client.query(query)\n",
    "\n",
    "    df = job.to_dataframe()\n",
    "    df.to_parquet(output_path)\n",
    "\n",
    "\n",
    "# query = comp.create_component_from_func(\n",
    "#     _query,\n",
    "#     base_image=\"python:3.9-slim\",\n",
    "#     packages_to_install=[\"google-cloud-bigquery\", \"pandas\", \"pyarrow\"],\n",
    "#     output_component_file=\"query.yaml\",\n",
    "# )\n",
    "\n",
    "\n",
    "train_model = comp.load_component_from_text(\n",
    "    \"\"\"\n",
    "name: Train model\n",
    "description: Trains our model\n",
    "\n",
    "inputs:\n",
    "- {name: train_dataset, type: Parquet, description: 'Train dataset'}\n",
    "\n",
    "outputs:\n",
    "- {name: model, type: Model, description: 'Output model'}\n",
    "\n",
    "implementation:\n",
    "  container:\n",
    "    image: europe-west1-docker.pkg.dev/cde-ds-enablement-8k1r/vertex/penguin_model\n",
    "    command: [\n",
    "      penguin-model, \n",
    "      {inputPath: train_dataset},\n",
    "      {outputPath: model}\n",
    "    ]\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "@component(\n",
    "    base_image=\"python:3.9-slim\",\n",
    "    output_component_file=\"eval_model.yaml\"\n",
    ")\n",
    "def eval_model(model_path: InputPath(\"Model\"), metrics: Output[Metrics]) -> NamedTuple(\n",
    "    'EvalModelOutput',\n",
    "    [\n",
    "      ('roc', float)\n",
    "    ]):\n",
    "  print(model_path)\n",
    "\n",
    "  metrics.log_metric(\"roc\", 0.9)\n",
    "  \n",
    "\n",
    "@kfp.dsl.pipeline(name=\"penguin\")\n",
    "def pipeline():\n",
    "\n",
    "    query_task = query(\n",
    "        \"SELECT * FROM bigquery-public-data.ml_datasets.penguins\",\n",
    "        project_id=\"cde-ds-enablement-8k1r\",\n",
    "    )\n",
    "\n",
    "    train_task = (\n",
    "        train_model(query_task.outputs[\"output_path\"])\n",
    "        # TODO: Figure out how to set constaints in the v2 API.\n",
    "        # Docs: https://www.kubeflow.org/docs/distributions/gke/pipelines/enable-gpu-and-tpu/\n",
    "        # .set_gpu_limit(1).add_node_selector_constraint(\n",
    "        #     \"cloud.google.com/gke-accelerator\", \"nvidia-tesla-k80\"\n",
    "        # )\n",
    "    )\n",
    "\n",
    "    eval_task = eval_model(train_task.outputs[\"model\"])\n",
    "\n",
    "    # model_upload_op = gcc_aip.ModelUploadOp(\n",
    "    #     project=project,\n",
    "    #     display_name=model_display_name,\n",
    "    #     artifact_uri=WORKING_DIR,\n",
    "    #     serving_container_image_uri=serving_container_image_uri,\n",
    "    #     # serving_container_environment_variables={\"NOT_USED\": \"NO_VALUE\"},\n",
    "    # )\n",
    "    # model_upload_op.after(train_task)\n",
    "\n",
    "\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=pipeline,\n",
    "    package_path=\"pipeline.json\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud.aiplatform.pipeline_jobs import PipelineJob\n",
    "\n",
    "job = PipelineJob(\n",
    "    display_name=\"penguins\",\n",
    "    enable_caching=False,\n",
    "    template_path=\"pipeline.json\",\n",
    "    parameter_values={},\n",
    "    pipeline_root=\"gs://cde-dse-penguin-artifacts/pipelines\",\n",
    "    location=\"europe-west1\",\n",
    ")\n",
    "\n",
    "job.run(service_account=\"pipeline-penguin@cde-ds-enablement-8k1r.iam.gserviceaccount.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job.run(\n",
    "    model_display_name=\"penguin-model\",\n",
    "    service_account=\"pipeline-penguin@cde-ds-enablement-8k1r.iam.gserviceaccount.com\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PipelineJob?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experimental.run_as_aiplatform_custom_job?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
